{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab289073-b2bf-43f7-9f96-7170d0de4a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database."
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1898421936.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    Ziel dieses Projekts war es, ein bestehendes Benchmarking-Tool, das ursprünglich zur Evaluierung von **ChatGPT** entwickelt wurde, so anzupassen und zu erweitern, dass es mit **Hugging Face-Modellen** funktioniert. Das Ziel war es, ein System zu schaffen, das deutsche Sprachmodelle von Hugging Face anhand verschiedener Metriken wie **BLEU**, **ROUGE**, **Hate Speech Detection** und **Word Connotation Recognition** bewertet. Diese Metriken bieten Einblick in die Leistung der Modelle in Bezug auf linguistische Qualität sowie der Erkennung von Vorurteilen, was für eine verantwortungsvolle Entwicklung von KI entscheidend ist.\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Projektdokumentation: Benchmarking von deutschen Sprachmodellen mit Hugging Face\n",
    "\n",
    "## Einleitung\n",
    "\n",
    "Ziel dieses Projekts war es, ein bestehendes Benchmarking-Tool, das ursprünglich zur Evaluierung von **ChatGPT** entwickelt wurde, so anzupassen und zu erweitern, dass es mit **Hugging Face-Modellen** funktioniert. Das Ziel war es, ein System zu schaffen, das deutsche Sprachmodelle von Hugging Face anhand verschiedener Metriken wie **BLEU**, **ROUGE**, **Hate Speech Detection** und **Word Connotation Recognition** bewertet. Diese Metriken bieten Einblick in die Leistung der Modelle in Bezug auf linguistische Qualität sowie der Erkennung von Vorurteilen, was für eine verantwortungsvolle Entwicklung von KI entscheidend ist.\n",
    "\n",
    "### Projektziel\n",
    "\n",
    "Das Hauptziel bestand darin, das Tool so umzubauen, dass es sich mit Hugging Face-Modellen verbindet und damit unabhängig von proprietären Modellen wie ChatGPT wird. Das Tool durchsucht automatisch verfügbare Modelle auf Hugging Face, führt vordefinierte Evaluierungen durch und stellt die Ergebnisse strukturiert dar. Folgende Bewertungen sind Teil des Benchmarks:\n",
    "\n",
    "1. **BLEU-Score**: Bewertet, wie gut das Modell Text generiert, indem es mit von Menschen verfassten Referenztexten verglichen wird.\n",
    "2. **ROUGE-Score**: Misst die Überlappung zwischen dem vom Modell generierten Text und dem Referenztext auf verschiedenen Ebenen (ROUGE-1, ROUGE-2, ROUGE-L).\n",
    "3. **Hate Speech Detection**: Testet die Fähigkeit des Modells, Hassrede zu erkennen und zu vermeiden.\n",
    "4. **Word Connotation Recognition**: Bewertet die Sensibilität des Modells gegenüber der Konnotation von Wörtern, um sicherzustellen, dass es subtile Vorurteile in der Sprache versteht.\n",
    "\n",
    "### Wichtige Schritte\n",
    "\n",
    "Das Projekt wurde in mehrere Sprints unterteilt, wobei jeder Sprint einen spezifischen Aspekt der Implementierung abdeckte:\n",
    "\n",
    "---\n",
    "\n",
    "## Sprint 1\n",
    "\n",
    "### Aufgaben\n",
    "\n",
    "- **Auslesen von Model-Cards via API**: Zuerst wurde die Hugging Face API verwendet, um die Metadaten der auf Hugging Face verfügbaren Modelle abzurufen.\n",
    "\n",
    "- **Testanbindung eines Hugging Face-Modells via API**: Als Proof of Concept wurde ein erstes Modell angebunden und erfolgreich getestet.\n",
    "\n",
    "- **Durchiterieren aller Model-Cards**: Alle verfügbaren Modelle wurden durchiteriert, um eine Liste der Model-Namen zu erstellen.\n",
    "\n",
    "- **Automatisierung des Auslesens aller Model-Cards**: Ein Skript wurde geschrieben, das automatisch die Informationen zu allen Modellen extrahiert.\n",
    "\n",
    "- **Erstellung von JSONs mit den Infos zu Sprache und Tasks**: Die extrahierten Model-Daten wurden in einer strukturierten JSON-Datei gespeichert (einzelne Model-Card_files), um Informationen über die unterstützten Sprachen und Aufgaben festzuhalten.\n",
    "\n",
    "---\n",
    "\n",
    "## Sprint 2\n",
    "\n",
    "### Aufgaben\n",
    "\n",
    "- **Popularität des Models einbeziehen**: In diesem Schritt wurde die Popularität der Modelle anhand der Anzahl der Downloads und Likes auf Hugging Face berücksichtigt, um eine fundierte Auswahl treffen zu können. (Ergebnisse werden sortiert.)\n",
    "  \n",
    "- **Möglichkeit zur Auswahl der Modelle, welche gescannt werden**: Es wurde eine Benutzereingabe entwickelt, die es ermöglicht, gezielt Modelle aus einer Liste auszuwählen, welche für den Benchmark gescannt und bewertet werden sollen. (Nummer des Modells auswählen)\n",
    "  \n",
    "- **Ergebnisse in Ergebnisfile speichern**: Die Benchmark-Ergebnisse eines händisch getesteten Modells wurde strukturiert in einer Datei gespeichert, um eine spätere Analyse und Auswertung zu erleichtern.\n",
    "  \n",
    "- **Ausgabe aller gefundenen und relevanten Modelle**: Eine vollständige Liste der gefundenen Modelle, die die gewünschten Sprachaufgaben unterstützen, wurde in der Benutzeroberfläche ausgegeben, damit die Nutzer schnell eine Übersicht erhalten.\n",
    "\n",
    "---\n",
    "\n",
    "## Sprint 3\n",
    "\n",
    "### Aufgaben\n",
    "\n",
    "- **Ergebnisse pro Test graphisch darstellen mit Streamlit**: Die Ergebnisse der durchgeführten Benchmark-Tests wurden in einem Streamlit-Dashboard grafisch aufbereitet und visualisiert. Dies ermöglichte es, die Leistung der Modelle übersichtlich in Form von Balkendiagrammen zu präsentieren.\n",
    "\n",
    "- **Möglichkeit schaffen mit dem Benchmark, ein ausgewähltes Modell zu testen, ohne hardgecodetes Modell (Automatisierung)**: Ein flexibles System wurde entwickelt, um beliebige Modelle aus Hugging Face auszuwählen und mit dem Benchmark zu testen. Dies ersetzt das vorherige System, das auf ein fest kodiertes Modell angewiesen war.\n",
    "\n",
    "### Herausforderungen\n",
    "\n",
    "Während der Implementierung traten einige unerwartete Hindernisse auf:\n",
    "\n",
    "- **Änderung der Architektur**: Die Einbindung der Ergebnis-Dateien funktionierte nicht wie geplant. Daher musste die Architektur des Benchmarks angepasst werden. Die Benchmark-Tests wurden so verändert, dass sie nun das Modell und den Tokenizer als Parameter übergeben bekommen. Anstatt Dateien zu erstellen, werden die Ergebnisse (Scores) direkt von den Methoden zurückgegeben.\n",
    "\n",
    "- **Schließung der Azure-GPT-API**: Während der Entwicklung wurde der Zugang zur Azure-GPT-API von Nortal unerwartet geschlossen. Dadurch war es nicht mehr möglich, dynamische Vergleiche mit ChatGPT-Scores durchzuführen. Stattdessen werden die Vergleiche nun mit den letzten verfügbaren statischen Daten von ChatGPT vorgenommen.\n",
    "\n",
    "---\n",
    "\n",
    "## Zusammenfassung\n",
    "\n",
    "Das Projekt hat erfolgreich ein Benchmarking-Tool für deutsche Sprachmodelle auf Hugging Face entwickelt, das Metriken wie BLEU, ROUGE, Hate Speech Detection und Word Connotation Recognition verwendet. Dieses Tool bietet eine umfassende Bewertung der Sprachmodelle in Bezug auf ihre sprachliche Qualität und ethische Sensibilität.\n",
    "\n",
    "\n",
    "## Potenzielle Weiterentwicklungen\n",
    "\n",
    "- **Refaktorierung weiterer Benchmark-Tests**: Eine sinnvolle Erweiterung wäre die Umgestaltung der verbleibenden Benchmark-Tests, sodass auch diese direkte Ergebnisse in Form von Scores zurückliefern und sowohl Modell als auch Tokenizer als Parameter übergeben. Diese Anpassung würde die Konsistenz des Frameworks stärken und die Handhabung der Tests weiter vereinfachen.\n",
    "\n",
    "- **Erweiterung der Metriken**: Um die Leistungsfähigkeit der Modelle noch präziser zu bewerten, könnte die Integration weiterer Metriken in Betracht gezogen werden. Dies würde ermöglichen, spezifischere Aussagen über die Modelle zu treffen und deren Stärken und Schwächen auf noch differenziertere Weise zu erfassen.\n",
    "\n",
    "- **Dynamische Vergleiche mit aktuellen Daten**: Sobald der Zugriff auf die GPT-API wiederhergestellt ist, könnte ein dynamischer Vergleich der getesteten Modelle mit aktuellen ChatGPT-Ergebnissen anstelle statischer Vergleichswerte implementiert werden. Dies würde die Relevanz und Aktualität der Benchmarks signifikant erhöhen.\n",
    "\n",
    "- **Ausweitung auf weitere Sprachen und Anwendungsfälle**: Eine langfristige Weiterentwicklung des Projekts könnte die Unterstützung weiterer Sprachen sowie zusätzlicher Anwendungsfälle umfassen. Dies würde den Anwendungsbereich des Benchmark-Tools deutlich erweitern und dessen Wert für die Bewertung von Sprachmodellen verschiedener Domänen steigern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829e3b9-0764-4f14-9a21-7798325478ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
